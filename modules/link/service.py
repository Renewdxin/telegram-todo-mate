import logging
import re
import html
from typing import Optional, Tuple, List

import aiohttp
from bs4 import BeautifulSoup

from modules.link.ai_service import AIService
from modules.link.repository import LinkRepository
from modules.database import SessionLocal
from modules.link.models import Link


async def fetch_title(url: str) -> Optional[str]:
    """é€šè¿‡ç½‘ç»œè¯·æ±‚è·å–ç½‘é¡µçš„æ ‡é¢˜"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, "html.parser")
                    if soup.title and soup.title.string:
                        return soup.title.string.strip()
    except Exception as e:
        # å¯æ ¹æ®éœ€è¦è®°å½•æ—¥å¿—
        return None


class LinkService:
    def __init__(self):
        self.repository = LinkRepository()
        self.ai_service = AIService()

    def clean_html(self, text: str) -> str:
        """æ¸…ç†HTMLæ ‡ç­¾å¹¶è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦"""
        # ç§»é™¤æ‰€æœ‰HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
        text = html.escape(text)
        return text

    async def extract_url_and_title(self, text: str) -> Tuple[str, Optional[str]]:
        """
        ä»æ–‡æœ¬ä¸­æå– URL å’Œæ ‡é¢˜ã€‚
        å¦‚æœç”¨æˆ·æ²¡æœ‰æä¾›æ ‡é¢˜ï¼Œåˆ™è‡ªåŠ¨è¯·æ±‚ç½‘é¡µæ¥æå–æ ‡é¢˜ã€‚
        æ”¯æŒæ ¼å¼ï¼š
        1. çº¯ URL
        2. æ ¼å¼å½¢å¦‚ "æ ‡é¢˜ URL"
        """
        url_pattern = r'https?://[^\s]+'
        url_match = re.search(url_pattern, text)
        if not url_match:
            raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„ URL")

        url = url_match.group()
        # å¦‚æœ URL å‰é¢çš„éƒ¨åˆ†å­˜åœ¨ï¼Œåˆ™è®¤ä¸ºæ˜¯æ ‡é¢˜
        title = text.replace(url, '').strip() or None

        # å¦‚æœæœªæä¾›æ ‡é¢˜ï¼Œåˆ™å°è¯•ä½¿ç”¨ AI ç”Ÿæˆ
        if not title:
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(url) as response:
                        if response.status == 200:
                            content = await response.text()
                            # ä½¿ç”¨ AI æœåŠ¡ç”Ÿæˆæ ‡é¢˜
                            title = await self.ai_service.generate_title(url, content)
            except Exception as e:
                logging.error(f"ç”Ÿæˆæ ‡é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                title = None

        return url, title

    async def save_link(self, user_id: int, text: str) -> str:
        """ä¿å­˜é“¾æ¥å¹¶è¿”å›æç¤ºä¿¡æ¯"""
        try:
            url, title = await self.extract_url_and_title(text)
            if title:
                title = self.clean_html(title)  # æ¸…ç†æ ‡é¢˜ä¸­çš„HTML
            db = SessionLocal()
            try:
                link = Link(
                    user_id=user_id,
                    url=url,
                    title=title,
                    is_read=False
                )
                db.add(link)
                db.commit()
                db.refresh(link)
                return f"âœ… é“¾æ¥å·²ä¿å­˜ï¼\nğŸ”— ID: {link.id}\nğŸ“ æ ‡é¢˜: {title if title else 'æ— æ ‡é¢˜'}"
            finally:
                db.close()
        except ValueError as e:
            return f"âŒ é”™è¯¯: {str(e)}"
        except Exception as e:
            logging.error(f"ä¿å­˜é“¾æ¥æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return "âŒ ä¿å­˜é“¾æ¥æ—¶å‘ç”Ÿé”™è¯¯"

    def get_unread_summary(self, user_id: int) -> str:
        """è·å–æœªè¯»é“¾æ¥ç»Ÿè®¡ä¿¡æ¯"""
        count = self.repository.get_unread_count(user_id)
        if count == 0:
            return "ğŸ“š æ‚¨ç°åœ¨æ²¡æœ‰æœªè¯»çš„é“¾æ¥"
        return f"ğŸ“š æ‚¨è¿˜æœ‰ {count} ä¸ªé“¾æ¥æœªè¯»"

    def format_link_info(self, link) -> str:
        """æ ¼å¼åŒ–é“¾æ¥ä¿¡æ¯"""
        result = f"ğŸ”— é“¾æ¥ {link.id}:\n"
        if link.title:
            result += f"ğŸ“ æ ‡é¢˜: {link.title}\n"
        result += f"ğŸŒ URL: {link.url}\n"
        if link.summary:
            result += f"ğŸ“‹ æ‘˜è¦: {link.summary}\n"
        result += f"â° ä¿å­˜æ—¶é—´: {link.created_at.strftime('%Y-%m-%d %H:%M')}"
        return result

    def get_random_unread_link(self, user_id: int) -> str:
        """è·å–éšæœºæœªè¯»é“¾æ¥"""
        link = self.repository.get_random_unread_link(user_id)
        if not link:
            return "ğŸ“­ æ²¡æœ‰æœªè¯»çš„é“¾æ¥"
        return self.format_link_info(link)

    def mark_as_read(self, link_id: int) -> str:
        """å°†é“¾æ¥æ ‡è®°ä¸ºå·²è¯»"""
        if self.repository.mark_as_read(link_id):
            return f"âœ… é“¾æ¥ {link_id} å·²æ ‡è®°ä¸ºå·²è¯»"
        return f"âŒ é“¾æ¥ {link_id} ä¸å­˜åœ¨"

    def update_summary(self, link_id: int, summary: str) -> str:
        """æ›´æ–°é“¾æ¥æ‘˜è¦"""
        if self.repository.update_summary(link_id, summary):
            return f"âœ… é“¾æ¥æ‘˜è¦å·²æ›´æ–°ï¼š\n\n{summary}"
        return f"âŒ æ›´æ–°æ‘˜è¦å¤±è´¥ï¼šé“¾æ¥ {link_id} ä¸å­˜åœ¨"

    def get_latest_unread_link(self, user_id: int) -> str:
        """è·å–æœ€æ–°æ·»åŠ çš„æœªè¯»é“¾æ¥ä¿¡æ¯"""
        link = self.repository.get_latest_unread_link(user_id)
        if not link:
            return "ğŸ“­ æ²¡æœ‰æœªè¯»çš„é“¾æ¥"
        return self.format_link_info(link)

    def get_unread_links(self, limit: int = 5) -> List[Link]:
        """è·å–æŒ‡å®šæ•°é‡çš„æœªè¯»é“¾æ¥"""
        db = SessionLocal()
        try:
            return (db.query(Link)
                    .filter(Link.is_read == False)
                    .order_by(Link.created_at.asc())
                    .limit(limit)
                    .all())
        finally:
            db.close()

    async def generate_summary(self, url: str) -> str:
        """ç”Ÿæˆé“¾æ¥å†…å®¹çš„æ‘˜è¦"""
        try:
            # è·å–ç½‘é¡µå†…å®¹
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    if response.status != 200:
                        return "æ— æ³•è·å–ç½‘é¡µå†…å®¹"
                    html_content = await response.text()

            # è§£æç½‘é¡µå†…å®¹
            soup = BeautifulSoup(html_content, 'html.parser')
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style"]):
                script.decompose()
            text = soup.get_text()
            
            # æ¸…ç†æ–‡æœ¬
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            # ä½¿ç”¨ AI ç”Ÿæˆæ‘˜è¦
            return await self.ai_service.generate_summary(url, text[:5000])
        except Exception as e:
            logging.error(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
            return "ç”Ÿæˆæ‘˜è¦æ—¶å‘ç”Ÿé”™è¯¯"
